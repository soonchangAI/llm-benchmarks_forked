{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "87323c74-8760-41b6-b593-a88c7cf11aa0",
   "metadata": {},
   "source": [
    "!wget https://raw.githubusercontent.com/mesolitica/malaysian-dataset/master/llm-benchmark/BM-pt3/BM-A-pt3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2f120f40",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# os.environ['CUDA_VISIBLE_DEVICES'] = '0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f4913978",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cybertron/anaconda3/envs/sgpt/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Loading checkpoint shards: 100%|██████████████████| 3/3 [03:05<00:00, 61.77s/it]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "#mesolitica/malaysian-llama2-7b-32k-instructions-v2\n",
    "tokenizer = AutoTokenizer.from_pretrained('mesolitica/mallam-5B-4096')\n",
    "# model = AutoModelForCausalLM.from_pretrained(\n",
    "#     'fpf-7b-instructions-16k-call-v2/checkpoint-9300', \n",
    "#     use_flash_attention_2 = True, \n",
    "#     torch_dtype = torch.float16,\n",
    "#     device_map=\"cuda:0\"\n",
    "# )\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    'mesolitica/mallam-5B-4096', \n",
    "    #use_flash_attention_2 = True, \n",
    "    torch_dtype = torch.float16,\n",
    "    device_map=\"auto\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "69b595ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f13823ddc10>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import os\n",
    "import random\n",
    "import json\n",
    "random.seed(404)\n",
    "torch.manual_seed(404)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e95823d-6921-49bd-b336-42c13468b5dc",
   "metadata": {},
   "source": [
    "data_folder = '/media/cybertron/fa54fcb6-b5e1-492e-978a-6389519c168a/llm-benchmarks/BM-PT3/data/'\n",
    "with open(data_folder + 'BM-A-pt3') as fopen:\n",
    "    text = fopen.read()\n",
    "print(text.split('no: '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7b8b065c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data_folder = '/media/cybertron/fa54fcb6-b5e1-492e-978a-6389519c168a/llm-benchmarks/sejarah-spm/data/'\n",
    "with open(data_folder+ 'sejarah_spm') as fopen:\n",
    "    text = fopen.read()\n",
    "    \n",
    "questions = []\n",
    "for t in text.split('no: ')[1:]: # split by \"no\"\n",
    "    t = t.strip()\n",
    "    no = t.split('\\n')[0] # \"split by newline to get index'\n",
    "    soalan = t.split('soalan:')[1].split('jawapan:')[0].strip()\n",
    "    jawapan = t.split('jawapan: ')[1].split(',')[0].strip()\n",
    "    data = {\n",
    "        'no': no,\n",
    "        'soalan': soalan,\n",
    "        'jawapan': jawapan,\n",
    "    }\n",
    "    questions.append(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d956f1ba-f570-4233-81e7-e6cefa5c00ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'no': '1',\n",
       " 'soalan': 'Apakah yang dimaksudkan dengan wilayah pengaruh?\\nA. Kawasan tanah jajahan dan takluk raja\\nB. Kawasan yang dinaungi oleh raja dan kerabatnya\\nC. Kawasan yang rakyatnya menerima dan memperakui pemerintahan seseorang raja\\nD. Kawasan yang rakyatnya menganggap raja sebagai tuhan atau wakil tuhan',\n",
       " 'jawapan': 'C'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "questions[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0fa9b493",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19}\n"
     ]
    }
   ],
   "source": [
    "arange = set(range(len(questions)))\n",
    "print(arange)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1ddc8579",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_prompt(row, answer = False): # convert question to prompt \n",
    "    if answer: # include answer in prompt\n",
    "        prompt = f\"\"\" \n",
    "soalan: {row['soalan']}\n",
    "jawapan: {row['jawapan']}\n",
    "    \"\"\"\n",
    "    else: # do not include answer in prompt\n",
    "        prompt = f\"\"\"\n",
    "soalan: {row['soalan']}\n",
    "jawapan:\n",
    "    \"\"\"\n",
    "    return prompt.strip()\n",
    "\n",
    "def parse_mistral_chat(messages):\n",
    "\n",
    "    user_query = messages[-1]['content']\n",
    "\n",
    "    users, assistants = [], []\n",
    "    for q in messages[:-1]:\n",
    "        if q['role'] == 'user':\n",
    "            users.append(q['content'])\n",
    "        elif q['role'] == 'assistant':\n",
    "            assistants.append(q['content'])\n",
    "\n",
    "    texts = ['<s>']\n",
    "    for u, a in zip(users, assistants):\n",
    "        texts.append(f'[INST] {u.strip()} [/INST]{a.strip()}</s> ')\n",
    "\n",
    "    texts.append(f'[INST] {user_query.strip()} [/INST]')\n",
    "    prompt = ''.join(texts).strip()\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2b0a6c8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                    | 0/20 [00:00<?, ?it/s]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "  5%|██▏                                         | 1/20 [00:10<03:27, 10.93s/it]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      " 10%|████▍                                       | 2/20 [00:20<02:57,  9.86s/it]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      " 15%|██████▌                                     | 3/20 [00:29<02:42,  9.54s/it]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      " 20%|████████▊                                   | 4/20 [00:39<02:34,  9.65s/it]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      " 25%|███████████                                 | 5/20 [00:48<02:23,  9.58s/it]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      " 30%|█████████████▏                              | 6/20 [00:57<02:13,  9.52s/it]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      " 35%|███████████████▍                            | 7/20 [01:07<02:02,  9.45s/it]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      " 40%|█████████████████▌                          | 8/20 [01:16<01:52,  9.39s/it]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      " 45%|███████████████████▊                        | 9/20 [01:25<01:43,  9.38s/it]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      " 50%|█████████████████████▌                     | 10/20 [01:35<01:33,  9.38s/it]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      " 55%|███████████████████████▋                   | 11/20 [01:44<01:23,  9.32s/it]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      " 60%|█████████████████████████▊                 | 12/20 [01:53<01:14,  9.29s/it]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      " 65%|███████████████████████████▉               | 13/20 [02:02<01:04,  9.26s/it]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      " 70%|██████████████████████████████             | 14/20 [02:11<00:55,  9.25s/it]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      " 75%|████████████████████████████████▎          | 15/20 [02:21<00:45,  9.18s/it]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      " 80%|██████████████████████████████████▍        | 16/20 [02:30<00:36,  9.13s/it]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      " 85%|████████████████████████████████████▌      | 17/20 [02:39<00:27,  9.16s/it]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      " 90%|██████████████████████████████████████▋    | 18/20 [02:48<00:18,  9.17s/it]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      " 95%|████████████████████████████████████████▊  | 19/20 [02:57<00:09,  9.12s/it]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "100%|███████████████████████████████████████████| 20/20 [03:06<00:00,  9.32s/it]\n"
     ]
    }
   ],
   "source": [
    "for i in tqdm(range(len(questions))):\n",
    "    \n",
    "    prompts = []\n",
    "    prompts.append(convert_prompt(questions[i])) # convert the question to prompt\n",
    "    prompt = '\\n\\n'.join(prompts)\n",
    "    messages = [\n",
    "        {'role': 'user', 'content': prompt}\n",
    "    ]\n",
    "    prompt = parse_mistral_chat(messages)\n",
    "    # print(prompt)\n",
    "    # break\n",
    "    inputs = tokenizer([prompt], return_tensors='pt', add_special_tokens=False).to('cuda') # get input ids \n",
    "    repeat = []\n",
    "    for _ in range(5): # for each question repeat 5 times\n",
    "        try:\n",
    "            generate_kwargs = dict(\n",
    "                inputs,\n",
    "                max_new_tokens=50,\n",
    "                top_p=0.95,\n",
    "                top_k=50,\n",
    "                temperature=0.9,\n",
    "                do_sample=True,\n",
    "                num_beams=1,\n",
    "                repetition_penalty=1.05,\n",
    "            )\n",
    "            r = model.generate(**generate_kwargs)\n",
    "            splitted = tokenizer.decode(r[0]).split('[/INST]')[1].strip().replace('</s>', '').replace('.', '').replace(',', '').strip().split()\n",
    "            splitted = [t for t in splitted if len(t) == 1 and t in {'A', 'B', 'C', 'D'}]\n",
    "            repeat.append(splitted[0])\n",
    "    \n",
    "        except Exception as e:\n",
    "            pass\n",
    "    \n",
    "    questions[i]['output'] = repeat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2f78c1e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def most_common(l):\n",
    "    return max(set(l), key=l.count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4b46a539",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "35.0"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered = [q for q in questions if 'output' in q and len(q['output'])]\n",
    "correct = 0\n",
    "for q in filtered:\n",
    "    correct += most_common(q['output']) == q['jawapan']\n",
    "(correct / len(filtered)) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bc8192fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "\n",
    "prompt = '<s>nama saya'\n",
    "inputs = tokenizer([prompt], return_tensors='pt', add_special_tokens=False).to('cuda')\n",
    "\n",
    "generate_kwargs = dict(\n",
    "    inputs,\n",
    "    max_new_tokens=512,\n",
    "    top_p=0.95,\n",
    "    top_k=50,\n",
    "    temperature=0.9,\n",
    "    do_sample=True,\n",
    "    num_beams=1,\n",
    "    repetition_penalty=1.05,\n",
    ")\n",
    "r = model.generate(**generate_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9dba4462-c64d-4338-a7b4-faf21bffad64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[    1,  2868,   689,   270,   454,  3010, 22722, 14277,  1332,  2018,\n",
       "           353,  3698,    81,   663,     2,     1,  2231,    49, 27831,   282,\n",
       "          1836,  2038,    93,   280,  2231,    49,   507,  8509, 18520,  2231,\n",
       "         23959,    87,    66,    73,   443,  8623,  2231,    55,   333, 10285,\n",
       "            78,   394,   476,   527,    17,     2,     1,   332,   591,  4625,\n",
       "          3905,  6932, 11846,  2082,  4758,  2549, 13391, 27102,   273, 18040,\n",
       "         20848,    17,   315,    83,  2457, 18959,    34,  2734,  1878,   802,\n",
       "         14738,  3332,   341, 15843, 19412,   802,  3225,  7287,  1515,    87,\n",
       "            17,  3865,    18,    24,    54,    91,    21,    46,    23,    44,\n",
       "            27,    84,    40,     2,     1,  2231,  5131,    23,    73,    22,\n",
       "           538,    75,    22,    73,    22,    92,    20,  2231, 11818,  1413,\n",
       "         19908,  2231, 30514,  2060,    66,  8111,  2231, 11818,  1413, 19908,\n",
       "          2231,  2341,    90,   507,    93,  1150,   278,   292,  1811,   802,\n",
       "          5357,  1748,     4,  2231,    80,  1606,    77,   280,  1606,    72,\n",
       "           265, 10546, 13391,  3225, 18068,   802,   812,  3733,   289,    15,\n",
       "         13157,  3608, 13406,    17,  7287,  1515,    87,    17,  3865,    18,\n",
       "            61,    74,    53,    21,    60,    58,    79,    49,    48,    19,\n",
       "             2,     1,  2231,    47,    92,  8044,    66,   516,  5578,  1219,\n",
       "          2231,    36,  1300,    68,  1904,    41,   468,   309,  2231,   285,\n",
       "         12478,  2495,   287,   357,  2338,  3530,  1207,     4,     2,     1,\n",
       "          2231,    36, 10714, 28662,    43,   546,  2231,    48,  9072,    45,\n",
       "          1908,  2231, 28935,  8090, 11541,  5854,  3355,  6371,   628, 12472,\n",
       "            16,  3754,  5627, 23360, 10697,   313,   738,   700, 11900, 17758,\n",
       "            17,  7287,  1515,    87,    17,  3865,    18,    28,    59,    24,\n",
       "            88,    41,    56,    82,    25,    57,    58,     2,     1,  2231,\n",
       "            75,  1355,  2434, 23687,  1355, 29183,  2231,    71,   578,  4250,\n",
       "            68,   277,    85,  3379,  2231,    81,   306,   267,  2281,   670,\n",
       "          8019,  4250,  2231,    44,   830, 12515, 17783,   373,  2231,  1037,\n",
       "         21332,    49,   490,  8884, 29529, 11541,   341,    87, 12043, 10483,\n",
       "          1185,   802,    17, 22341, 15812,    86,   325,  4865,  2043,     4,\n",
       "             2,     1,  2231,    37,  7703,   404,    47,  1274,  2374,   292,\n",
       "           932,  1352,  3856,   539,    88,    17, 24862,  4139,  1822,  2283,\n",
       "          1293,  3224,  1185,    16,  1736,  5324,   807,  2171,   418,   293,\n",
       "           324,   389,   275,     2,     1,  2231,    66,    69,  1683, 20305,\n",
       "           584, 18798,   315,  5158, 17915,    88,    17,   383,  9892,   320,\n",
       "            74,   715,     2,     1,  2231,    80,  2625,   563,   297, 29620,\n",
       "          2231,    73,  9785,   299,   563,   297, 29620,   808,   304,  6446,\n",
       "           738,   324,   438,  3333,     2,     1,  2231,    69,   285, 14555,\n",
       "            81, 13855,   795,  2825,   410, 15029,  1384,   888,  1608,  2561,\n",
       "          1608,     2,     1,  2231,    75,  1643, 12507,  1560,  4250,    68,\n",
       "         14365,   468, 18798,   738, 28750,    16,  8090,   888, 11354,  5346,\n",
       "           449,  6179,    16, 13746,   901,     2,     1,  2231,    51, 31822,\n",
       "            45, 11391,    47,  3201,   292,  3418, 14738, 29809,  2932,  3225,\n",
       "         13682, 28580,    17,   625,    74,  1330,    21,   273,   879, 13843,\n",
       "           552, 11283,    17,   357,   316,   264, 18522,   269,    74,   274,\n",
       "          5121,  1331, 17865, 15962,    17,     2,     1,  2231,    73,   380,\n",
       "            93,  1150,  2221,   394,  1637,  1352,    17,     2,     1,  2231,\n",
       "            69,  5456,    66,    77,  3394,  5164,  1673,  5334,  2871, 14685,\n",
       "          2473,   260,     2,     1,  2231,    44, 19626, 14873,  2955,   278,\n",
       "          3105, 11806,  4555,   428,  3302,  2407,  1352, 11355,     2,     1,\n",
       "           369,  1564,   564,   431,   276,  5719,   265, 14473,  2568, 11878,\n",
       "            38,  2659,  4570,  2231,    76]], device='cuda:0')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "aac6226a-e1a8-473b-9781-e30985efbb6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s> nama saya muhammad azfar bin zulkarnain</s><s> @NaufalAntezem @NanaMustafa @twt_fotografi @Tiktokk__ Wow.</s><s> Klu dulu suka ikut trend sebab tengok ramai org tweet psl tudung. Tp skrg? Aku rasa dia nk kena rotan sbb dia yg https://t.co/5Sx2K4I8qE</s><s> @sy4f3ysh3f3y1 @AzminAli @Ayahanda_MD @AzminAli @drwanazizah Suruh dia jawab sendiri! @mijjemije tuduh org yg fitnah dia sebagai rasis, gila kuasa etc. https://t.co/ZgR2YWlNM0</s><s> @Lynn_ibrahim76 @AinaaAlFayed @adibahnoor Dengar cerita ni!</s><s> @AzzizzHadi @MKNJPM @PDRMsia tolong ambil tindakan segera kepada pemandu-pemandu Grabcar yang tak ada common sense. https://t.co/9X5uFUo6VW</s><s> @hazieqaazmn @dilaaaaasrdn @nurinshafiqaa @ImanAbdulRahim @InjangNation Nak mintak tolong rt utk rezeki anak dia. Thankssss in advance!</s><s> @BoongaLavender Saja nak bagitau. Korang jangan buat macam tu kat anak-anak nanti boleh jadi geng cicak</s><s> @_bloominglyyy Tungguuuu. Rindu hg juga</s><s> @misschelsea @fckingchelsea17 Mcm tak caya je</s><s> @badrulnaim1898 Hahahaha aku dah mula dah</s><s> @hanieyraaaa Yeayyy tak sia-sia aku pilih awak untuk kawan-kawan lain</s><s> @PejuangJlnLurus Susah nk dpt lelaki yg mcm nie. Yg baik2 plak jarang lekat. Duk dkt sg buluh pun dh okay.</s><s> @farizizwan__ Saya nak.</s><s> @bayu_joo Itu baru nyium bau pandan</s><s> @IzatSyafiah Taknak beli.. Tapi kalau nak jugak</s><s> Come on la korg tgk #BukanCintaAku @i\n"
     ]
    }
   ],
   "source": [
    "splitted = tokenizer.decode(r[0])\n",
    "print(splitted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1115e52e-d1cd-4672-a1e9-3671c4a348d2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
